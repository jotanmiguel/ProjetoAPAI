{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5d90e1",
   "metadata": {},
   "source": [
    "# Projeto: Impacto do Airbnb em Fatores Urbanos em Portugal (REVER AINDA)\n",
    "Este notebook faz parte da Fase 1 do projeto da unidade curricular.\n",
    "\n",
    "## Objetivos:\n",
    "- Integrar dados de diferentes fontes (Airbnb, INE, etc.)\n",
    "- Analisar a relação entre densidade de alojamentos locais, população e rendas\n",
    "- Implementar técnicas de data cleaning, schema integration e identity resolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d476029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Leitura dos datasets\n",
    "listings = pd.read_csv(r'dados\\portugal_listings.csv')\n",
    "rendas = pd.read_json(r'dados\\rendasm2.json')\n",
    "rendas = pd.DataFrame(rendas['Dados'][0]['2023'])\n",
    "densidade_pop = pd.read_json(r'dados\\densidadePopulacional.json')\n",
    "densidade_pop = pd.DataFrame(densidade_pop['Dados'][0]['2022'])\n",
    "densidade_aloj = pd.read_json(r'dados\\densidadealojamentosm2.json')\n",
    "densidade_aloj = pd.DataFrame(densidade_aloj['Dados'][0]['2021'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027d1b6",
   "metadata": {},
   "source": [
    "## 3. Data Profiling (ADICIONAR MAIS DETALHES)\n",
    "\n",
    "- Número de entradas\n",
    "- Tipos de dados\n",
    "- Percentagem de valores nulos\n",
    "- Cardinalidade de colunas relevantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7075979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 135536 entries, 0 to 135535\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   Price        135236 non-null  float64\n",
      " 1   District     135536 non-null  object \n",
      " 2   City         135536 non-null  object \n",
      " 3   Town         135534 non-null  object \n",
      " 4   Unnamed: 4   0 non-null       float64\n",
      " 5   Unnamed: 5   0 non-null       float64\n",
      " 6   Unnamed: 6   0 non-null       float64\n",
      " 7   Unnamed: 7   0 non-null       float64\n",
      " 8   Unnamed: 8   0 non-null       float64\n",
      " 9   Unnamed: 9   0 non-null       float64\n",
      " 10  Unnamed: 10  0 non-null       float64\n",
      " 11  Unnamed: 11  0 non-null       float64\n",
      " 12  Unnamed: 12  0 non-null       float64\n",
      " 13  Unnamed: 13  0 non-null       float64\n",
      "dtypes: float64(11), object(3)\n",
      "memory usage: 14.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Price             300\n",
       "District            0\n",
       "City                0\n",
       "Town                2\n",
       "Unnamed: 4     135536\n",
       "Unnamed: 5     135536\n",
       "Unnamed: 6     135536\n",
       "Unnamed: 7     135536\n",
       "Unnamed: 8     135536\n",
       "Unnamed: 9     135536\n",
       "Unnamed: 10    135536\n",
       "Unnamed: 11    135536\n",
       "Unnamed: 12    135536\n",
       "Unnamed: 13    135536\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings.info()\n",
    "listings.describe()\n",
    "listings.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9dca7",
   "metadata": {},
   "source": [
    "portugal_listings.csv\n",
    "\n",
    "| Coluna                  | Tipo de Dado | Valores Não Nulos | Valores Nulos | % Nulos | Ação Planeada               |\n",
    "|-------------------------|--------------|-------------------|----------------|---------|-----------------------------|\n",
    "| `Price`                 | float64      | 135236            | 300            | 0.22%   | Remover nulos ou imputar    |\n",
    "| `District`              | object       | 135536            | 0              | 0.00%   | Manter                      |\n",
    "| `City`                  | object       | 135536            | 0              | 0.00%   | Manter                      |\n",
    "| `Town`                  | object       | 135534            | 2              | 0.001%  | Imputar com `City` ou remover |\n",
    "| `Unnamed: 4` a `Unnamed: 13` | float64 | 0              | 135536         | 100%    | Remover todas               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3488cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 638 entries, 0 to 637\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   geocod           638 non-null    object\n",
      " 1   geodsg           638 non-null    object\n",
      " 2   ind_string       638 non-null    object\n",
      " 3   valor            431 non-null    object\n",
      " 4   sinal_conv       207 non-null    object\n",
      " 5   sinal_conv_desc  207 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 30.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "geocod               0\n",
       "geodsg               0\n",
       "ind_string           0\n",
       "valor              207\n",
       "sinal_conv         431\n",
       "sinal_conv_desc    431\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rendas.info()\n",
    "rendas.describe()\n",
    "rendas.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96956d9c",
   "metadata": {},
   "source": [
    "rendasm2.json\n",
    "\n",
    "| Coluna            | Tipo de Dado | Valores Não Nulos | Valores Nulos | % Nulos | Ação Planeada                                 |\n",
    "|-------------------|--------------|-------------------|----------------|---------|-----------------------------------------------|\n",
    "| `geocod`          | object       | 638               | 0              | 0.00%   | Manter                                        |\n",
    "| `geodsg`          | object       | 638               | 0              | 0.00%   | Usar como chave para junção                   |\n",
    "| `ind_string`      | object       | 638               | 0              | 0.00%   | Ignorar                                       |\n",
    "| `valor`           | object       | 431               | 207            | 32.45%  | Eliminar ou ignorar entradas nulas            |\n",
    "| `sinal_conv`      | object       | 207               | 431            | 67.55%  | Apenas explicativo (motivo da ausência de valor) |\n",
    "| `sinal_conv_desc` | object       | 207               | 431            | 67.55%  | Ignorar ou mover para anexo                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75114dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344 entries, 0 to 343\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   geocod      344 non-null    object\n",
      " 1   geodsg      344 non-null    object\n",
      " 2   ind_string  344 non-null    object\n",
      " 3   valor       344 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 10.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "geocod        0\n",
       "geodsg        0\n",
       "ind_string    0\n",
       "valor         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "densidade_pop.info()\n",
    "densidade_pop.describe()\n",
    "densidade_pop.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7934a7",
   "metadata": {},
   "source": [
    "densidadePopulacional.json\n",
    "\n",
    "| Coluna       | Tipo de Dado | Valores Não Nulos | Valores Nulos | % Nulos | Ação Planeada                     |\n",
    "|--------------|--------------|-------------------|----------------|---------|-----------------------------------|\n",
    "| `geocod`     | object       | 344               | 0              | 0.00%   | Manter                            |\n",
    "| `geodsg`     | object       | 344               | 0              | 0.00%   | Usar como chave                   |\n",
    "| `ind_string` | object       | 344               | 0              | 0.00%   | Ignorar                           |\n",
    "| `valor`      | object       | 344               | 0              | 0.00%   | Converter para float              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfadba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3439 entries, 0 to 3438\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   geocod      3439 non-null   object\n",
      " 1   geodsg      3439 non-null   object\n",
      " 2   ind_string  3439 non-null   object\n",
      " 3   valor       3439 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 107.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "geocod        0\n",
       "geodsg        0\n",
       "ind_string    0\n",
       "valor         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "densidade_aloj.info()\n",
    "densidade_aloj.describe()\n",
    "densidade_aloj.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077fe84d",
   "metadata": {},
   "source": [
    "densidadelojamentosm2.json\n",
    "\n",
    "| Coluna       | Tipo de Dado | Valores Não Nulos | Valores Nulos | % Nulos | Ação Planeada                     |\n",
    "|--------------|--------------|-------------------|----------------|---------|-----------------------------------|\n",
    "| `geocod`     | object       | 344               | 0              | 0.00%   | Manter                            |\n",
    "| `geodsg`     | object       | 344               | 0              | 0.00%   | Usar como chave                   |\n",
    "| `ind_string` | object       | 344               | 0              | 0.00%   | Ignorar                           |\n",
    "| `valor`      | object       | 344               | 0              | 0.00%   | Converter para float              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf62145",
   "metadata": {},
   "source": [
    "### Conclusões do Data Profiling\n",
    "\n",
    "Após a leitura e análise dos datasets, foi possível identificar a seguinte situação:\n",
    "\n",
    "- Os dados extraídos dos ficheiros `.json` foram corretamente convertidos em tabelas pandas a partir da chave `\"Dados\"`.\n",
    "- As colunas `valor`, apesar de estarem no formato `object`, podem ser convertidas para `float` após a substituição da vírgula pelo ponto decimal.\n",
    "- No dataset das rendas (`rendasm2.json`), foi detetada uma percentagem significativa de valores nulos (~32%) na coluna `valor`, que deverão ser removidos ou imputados.\n",
    "- Todos os datasets possuem a coluna `geodsg`, que será usada como **chave principal de junção**.\n",
    "- A coluna `geocod` foi mantida por agora, uma vez que poderá ser útil para cruzamentos futuros com dados geográficos oficiais.\n",
    "- Os campos como `ind_string`, `sinal_conv` e `sinal_conv_desc` foram considerados auxiliares e não serão usados na análise principal.\n",
    "\n",
    "**Próximos passos:**\n",
    "- Conversão das colunas `valor` para `float`.\n",
    "- Remoção dos registos com valores nulos nas variáveis principais.\n",
    "- Normalização de nomes para futura integração entre datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed5712",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9fe5e",
   "metadata": {},
   "source": [
    "### Data Cleaning & Preparation (REVER)\n",
    "\n",
    "Após a fase de data profiling, foi identificado que os datasets apresentam estruturas heterogéneas e inconsistências nos tipos de dados e formatação. Esta etapa tem como objetivo preparar os dados para integração e análise, assegurando a sua consistência e limpeza.\n",
    "\n",
    "#### Principais problemas detetados:\n",
    "\n",
    "- As colunas com valores numéricos estão no formato `string`;\n",
    "- Alguns campos contêm separadores decimais diferentes, `\".\"` e `\",\"`\n",
    "- Existem registos com valores nulos, especialmente em colunas importantes `Price` e `valor`;\n",
    "- Nomes (`District`, `Town`, `City`) contêm acentos, letras maiúsculas/minúsculas.\n",
    "- **(PODE HAVER MAIS)**\n",
    "\n",
    "#### Objetivos desta fase:\n",
    "\n",
    "1. **Converter colunas com números para `float`**;\n",
    "2. **Remover registos com valores nulos ou inválidos nas variáveis principais**;\n",
    "3. **Normalizar nomes de localidades** para permitir junções consistentes entre datasets:\n",
    "   - Remoção de acentos (se aplicável);\n",
    "   - Conversão para minúsculas;\n",
    "   - Eliminação de espaços antes/depois do texto;\n",
    "4. **Criar colunas normalizadas (`*_norm`)** que servirão como chaves comuns para integração.\n",
    "\n",
    "Com estas operações, os dados ficarão prontos para a fase seguinte de **integração de esquemas e resolução de identidade**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15f425cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\joaom\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56a3387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar nomes\n",
    "def normalizar(texto):\n",
    "    return unidecode.unidecode(str(texto).strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e757a-df78-420d-a09e-3ebccca7bf26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['Price', 'District', 'City', 'Town', 'Unnamed: 4', 'Unnamed: 5',\n",
      "       'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10',\n",
      "       'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13'],\n",
      "      dtype='object')\n",
      "Nº de valores nulos em 'Price': 300\n",
      "      Price   District                  City  \\\n",
      "0  780000.0  Vila Real              Valpaços   \n",
      "1  223000.0       Faro  São Brás de Alportel   \n",
      "2  228000.0       Faro  São Brás de Alportel   \n",
      "3  250000.0       Faro  São Brás de Alportel   \n",
      "4  250000.0       Faro  São Brás de Alportel   \n",
      "\n",
      "                               Town District_norm             City_norm  \\\n",
      "0  Carrazedo de Montenegro e Curros     vila real              valpacos   \n",
      "1              São Brás de Alportel          faro  sao bras de alportel   \n",
      "2              São Brás de Alportel          faro  sao bras de alportel   \n",
      "3              São Brás de Alportel          faro  sao bras de alportel   \n",
      "4              São Brás de Alportel          faro  sao bras de alportel   \n",
      "\n",
      "                          Town_norm  \n",
      "0  carrazedo de montenegro e curros  \n",
      "1              sao bras de alportel  \n",
      "2              sao bras de alportel  \n",
      "3              sao bras de alportel  \n",
      "4              sao bras de alportel  \n",
      "Número total de registos limpos: 135236\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "\n",
    "file_path = 'dados/portugal_listings.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Verificar colunas existentes\n",
    "print(\"Columns in the DataFrame:\", df.columns)\n",
    "\n",
    "# Remover colunas 'Unnamed'\n",
    "df = df.drop(columns=[col for col in df.columns if \"Unnamed\" in col])\n",
    "\n",
    "# Converter 'Price' para float (números)\n",
    "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')  # erros → NaN\n",
    "\n",
    "# Verificar valores nulos após conversão\n",
    "print(\"Nº de valores nulos em 'Price':\", df['Price'].isna().sum())\n",
    "\n",
    "# Filtrar linhas com preço válido (> 0)\n",
    "df_filtered = df[df['Price'].notna() & (df['Price'] > 0)].copy()\n",
    "\n",
    "df_filtered['District_norm'] = df_filtered['District'].apply(normalizar)\n",
    "df_filtered['City_norm'] = df_filtered['City'].apply(normalizar)\n",
    "df_filtered['Town_norm'] = df_filtered['Town'].apply(normalizar)\n",
    "\n",
    "print(df_filtered.head())\n",
    "print(\"Número total de registos limpos:\", len(df_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78999ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xlrd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Installing collected packages: xlrd\n",
      "Successfully installed xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b60a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas: Index(['codigo_municipio', 'regiao', 'codigo_regiao', 'distrito', 'municipio'], dtype='object')\n",
      "   codigo_municipio                       regiao codigo_regiao distrito  \\\n",
      "0               101             REGIÃO DE AVEIRO           16D   AVEIRO   \n",
      "1               102             REGIÃO DE AVEIRO           16D   AVEIRO   \n",
      "2               103             REGIÃO DE AVEIRO           16D   AVEIRO   \n",
      "3               104  ÁREA METROPOLITANA DO PORTO           11A   AVEIRO   \n",
      "4               105             REGIÃO DE AVEIRO           16D   AVEIRO   \n",
      "\n",
      "            municipio      municipio_norm distrito_norm  \\\n",
      "0              ÁGUEDA              agueda        aveiro   \n",
      "1  ALBERGARIA-A-VELHA  albergaria-a-velha        aveiro   \n",
      "2              ANADIA              anadia        aveiro   \n",
      "3              AROUCA              arouca        aveiro   \n",
      "4              AVEIRO              aveiro        aveiro   \n",
      "\n",
      "                   regiao_norm  \n",
      "0             regiao de aveiro  \n",
      "1             regiao de aveiro  \n",
      "2             regiao de aveiro  \n",
      "3  area metropolitana do porto  \n",
      "4             regiao de aveiro  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Caminho para o ficheiro\n",
    "file_path = 'dados/Areas_Freg_Conc_Dist_Pais_CAOP2019.xls'\n",
    "\n",
    "# Ler apenas a sheet dos conselhos (municípios)\n",
    "df_concelhos = pd.read_excel(file_path, sheet_name='Areas_Concelhos_CAOP2019')\n",
    "\n",
    "# Verificar colunas disponíveis\n",
    "df_concelhos = df_concelhos[['DICO', 'NUTSIII_DSG', 'NUTSIII_COD', 'DISTRITO_ILHA_DSG', 'CONCELHO_DSG']].copy()\n",
    "df_concelhos = df_concelhos.rename(columns={\n",
    "    'CONCELHO_DSG': 'municipio',\n",
    "    'NUTSIII_DSG': 'regiao',\n",
    "    'NUTSIII_COD': 'codigo_regiao',\n",
    "    'DISTRITO_ILHA_DSG': 'distrito',\n",
    "    'DICO': 'codigo_municipio'\n",
    "})\n",
    "print(\"Colunas:\", df_concelhos.columns)\n",
    "\n",
    "df_concelhos['municipio_norm'] = df_concelhos['municipio'].apply(normalizar)\n",
    "df_concelhos['distrito_norm'] = df_concelhos['distrito'].apply(normalizar)\n",
    "df_concelhos['regiao_norm'] = df_concelhos['regiao'].apply(normalizar)\n",
    "\n",
    "# Visualizar as primeiras linhas\n",
    "print(df_concelhos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66ab0ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['IndicadorCod', 'IndicadorDsg', 'MetaInfUrl', 'DataExtracao',\n",
      "       'DataUltimoAtualizacao', 'UltimoPref', 'Dados', 'Sucesso'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\joaom\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Converter 'Price' para float (números)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# erros → NaN\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Verificar valores nulos após conversão\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNº de valores nulos em \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n",
      "File \u001b[1;32mc:\\Users\\joaom\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\joaom\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Price'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "\n",
    "file_path = 'dados/rendasm2.json'\n",
    "df = pd.read_json(file_path)\n",
    "\n",
    "# Verificar colunas existentes\n",
    "print(\"Columns in the DataFrame:\", df.columns)\n",
    "\n",
    "# Remover colunas 'Unnamed'\n",
    "df = df.drop(columns=[col for col in df.columns if \"Unnamed\" in col])\n",
    "\n",
    "# Converter 'Price' para float (números)\n",
    "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')  # erros → NaN\n",
    "\n",
    "# Verificar valores nulos após conversão\n",
    "print(\"Nº de valores nulos em 'Price':\", df['Price'].isna().sum())\n",
    "\n",
    "# Filtrar linhas com preço válido (> 0)\n",
    "df_filtered = df[df['Price'].notna() & (df['Price'] > 0)].copy()\n",
    "\n",
    "# Normalizar nomes\n",
    "def normalizar(texto):\n",
    "    return unidecode.unidecode(str(texto).strip().lower())\n",
    "\n",
    "df_filtered['District_norm'] = df_filtered['District'].apply(normalizar)\n",
    "df_filtered['City_norm'] = df_filtered['City'].apply(normalizar)\n",
    "df_filtered['Town_norm'] = df_filtered['Town'].apply(normalizar)\n",
    "\n",
    "print(df_filtered.head())\n",
    "print(\"Número total de registos limpos:\", len(df_filtered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16762cb-c6a3-4e05-84ee-be50c8826751",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'concelhos.up.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mElementTree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mET\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mET\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconcelhos.up.xml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Parse XML file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m root \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mgetroot()  \u001b[38;5;66;03m# Get root element\u001b[39;00m\n\u001b[0;32m      6\u001b[0m objects_data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\joaom\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\xml\\etree\\ElementTree.py:1204\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(source, parser)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse XML document into element tree.\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m \n\u001b[0;32m   1197\u001b[0m \u001b[38;5;124;03m*source* is a filename or file object containing XML data,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \n\u001b[0;32m   1202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m tree \u001b[38;5;241m=\u001b[39m ElementTree()\n\u001b[1;32m-> 1204\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\joaom\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\xml\\etree\\ElementTree.py:558\u001b[0m, in \u001b[0;36mElementTree.parse\u001b[1;34m(self, source, parser)\u001b[0m\n\u001b[0;32m    556\u001b[0m close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 558\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m     close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'concelhos.up.xml'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('concelhos.up.xml')  # Parse XML file\n",
    "root = tree.getroot()  # Get root element\n",
    "\n",
    "objects_data = []\n",
    "for object_element in root.findall(\"Object\"):\n",
    "    object_dict = {}\n",
    "    for property_element in object_element.findall(\"Property\"):\n",
    "        name = property_element.get(\"Name\")\n",
    "        value = property_element.text\n",
    "        object_dict[name] = value\n",
    "    objects_data.append(object_dict)\n",
    "\n",
    "df = pd.DataFrame(objects_data)\n",
    "    \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f8dccdc-34ad-4645-b0e1-e43522c5440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['geocod', 'geodsg', 'ind_string', 'valor'], dtype='object')\n",
      "  RegionCode                                         RegionName DensityString  \\\n",
      "0     070513                                Torre de Coelheiros           2,1   \n",
      "1     070601                                            Cabrela           2,2   \n",
      "2     031003                                     Campo do Gerês           2,3   \n",
      "3     120302                                       Aldeia Velha           2,4   \n",
      "4     070525  União das freguesias de Nossa Senhora da Toure...           2,5   \n",
      "\n",
      "   DensityValue  \n",
      "0           2.1  \n",
      "1           2.2  \n",
      "2           2.3  \n",
      "3           2.4  \n",
      "4           2.5  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = 'dados/densidadealojamentosm2.json'  # Replace with the actual file path\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the 2021 data\n",
    "dados_2021 = data[0]['Dados']['2021']\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(dados_2021)\n",
    "\n",
    "# Print the column names to verify\n",
    "print(\"Columns in the DataFrame:\", df.columns)\n",
    "\n",
    "# Rename columns\n",
    "df = df.rename(columns={\n",
    "    'geocod': 'RegionCode',\n",
    "    'geodsg': 'RegionName',\n",
    "    'ind_string': 'DensityString',\n",
    "    'valor': 'DensityValue'  # Ensure this matches the actual column name\n",
    "})\n",
    "\n",
    "# Convert DensityValue to numeric\n",
    "df['DensityValue'] = pd.to_numeric(df['DensityValue'], errors='coerce')\n",
    "\n",
    "# Handle missing values (e.g., replace \"-\" with NaN)\n",
    "df['DensityValue'] = df['DensityValue'].replace('-', None)\n",
    "\n",
    "# Filter out rows with missing or non-applicable data\n",
    "df_filtered = df[df['DensityValue'].notna()]\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c59e7093-f407-44e3-bae9-334ea03574bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['geocod', 'geodsg', 'ind_string', 'valor'], dtype='object')\n",
      "  RegionCode     RegionName DensityString  DensityValue\n",
      "0    1500802       Alcoutim           4,3           4.3\n",
      "1    1840209        Mértola           4,8           4.8\n",
      "2    16H0505  Idanha-a-Nova           5,9           5.9\n",
      "3    1861203           Avis           6,2           6.2\n",
      "4    1861211       Monforte           7,1           7.1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = 'dados/densidadePopulacional.json'  # Replace with the actual file path\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the 2022 data\n",
    "dados_2022 = data[0]['Dados']['2022']\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(dados_2022)\n",
    "\n",
    "# Print the column names to verify\n",
    "print(\"Columns in the DataFrame:\", df.columns)\n",
    "\n",
    "# Rename columns\n",
    "df = df.rename(columns={\n",
    "    'geocod': 'RegionCode',\n",
    "    'geodsg': 'RegionName',\n",
    "    'ind_string': 'DensityString',\n",
    "    'valor': 'DensityValue'  # Ensure this matches the actual column name\n",
    "})\n",
    "\n",
    "# Convert DensityValue to numeric\n",
    "df['DensityValue'] = pd.to_numeric(df['DensityValue'], errors='coerce')\n",
    "\n",
    "# Handle missing values (e.g., replace \"-\" with NaN)\n",
    "df['DensityValue'] = df['DensityValue'].replace('-', None)\n",
    "\n",
    "# Filter out rows with missing or non-applicable data\n",
    "df_filtered = df[df['DensityValue'].notna()]\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dde95fce-c9ca-4f79-9222-4f6f10def11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['geocod', 'geodsg', 'ind_string', 'valor', 'sinal_conv',\n",
      "       'sinal_conv_desc'],\n",
      "      dtype='object')\n",
      "  RegionCode            RegionName RentString  RentValue SignalCode  \\\n",
      "0    11D0914  Vila Nova de Foz Côa       2,08       2.08        NaN   \n",
      "1    11E0410             Vila Flor       2,29       2.29        NaN   \n",
      "2    1960913              Trancoso       2,41       2.41        NaN   \n",
      "3    1960501              Belmonte       2,61       2.61        NaN   \n",
      "4    11C1302                 Baião       2,63       2.63        NaN   \n",
      "\n",
      "  SignalDescription  \n",
      "0               NaN  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = 'dados/rendasm2.json'  # Replace with the actual file path\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the 2023 data\n",
    "dados_2023 = data[0]['Dados']['2023']\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(dados_2023)\n",
    "\n",
    "# Print the column names to verify\n",
    "print(\"Columns in the DataFrame:\", df.columns)\n",
    "\n",
    "# Rename columns\n",
    "df = df.rename(columns={\n",
    "    'geocod': 'RegionCode',\n",
    "    'geodsg': 'RegionName',\n",
    "    'ind_string': 'RentString',\n",
    "    'valor': 'RentValue',  # Ensure this matches the actual column name\n",
    "    'sinal_conv': 'SignalCode',\n",
    "    'sinal_conv_desc': 'SignalDescription'\n",
    "})\n",
    "\n",
    "# Convert RentValue to numeric\n",
    "df['RentValue'] = pd.to_numeric(df['RentValue'], errors='coerce')\n",
    "\n",
    "# Handle missing values (e.g., replace \"-\" with NaN)\n",
    "df['RentValue'] = df['RentValue'].replace('-', None)\n",
    "\n",
    "# Filter out rows with missing or non-applicable data\n",
    "df_filtered = df[df['RentValue'].notna()]\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811e58b-41ab-4d5d-a1bd-0f4d52684d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
